{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from risparser import convert_ris_to_csv\n",
    "os.chdir(\"/automated-systematic-review-datasets/datasets/Van_de_Schoot_PTSD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert RIS files into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file: raw/schoot-lgmm-ptsd-initial.ris\n",
      "Number of articles: 6187\n",
      "Export file: csv/schoot-lgmm-ptsd-initial.csv\n",
      "Input file: raw/schoot-lgmm-ptsd-included-1.ris\n",
      "Number of articles: 363\n",
      "Export file: csv/schoot-lgmm-ptsd-included-1.csv\n",
      "Input file: raw/schoot-lgmm-ptsd-included-3.ris\n",
      "Number of articles: 8\n",
      "Export file: csv/schoot-lgmm-ptsd-included-3.csv\n",
      "Input file: raw/schoot-lgmm-ptsd-included-2.ris\n",
      "Number of articles: 38\n",
      "Export file: csv/schoot-lgmm-ptsd-included-2.csv\n"
     ]
    }
   ],
   "source": [
    "raw_files = [raw_files for raw_files in os.listdir(\"raw\") if raw_files.endswith(\".ris\")]\n",
    "os.makedirs(\"csv\", exist_ok=True)\n",
    "for ris_file in raw_files :\n",
    "    ris_fp = os.path.join(\"raw\", ris_file)\n",
    "    csv_fp = os.path.join(\"csv\", os.path.splitext(ris_file)[0]+\".csv\")\n",
    "    convert_ris_to_csv(ris_fp, csv_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data \n",
    "\n",
    "(Code from the R file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original papers\n",
    "os.chdir(\"csv\")\n",
    "all_data = pd.read_csv(\"schoot-lgmm-ptsd-initial.csv\")\n",
    "\n",
    "# included papers\n",
    "inc_data = pd.read_csv(\"schoot-lgmm-ptsd-included-2.csv\")\n",
    "\n",
    "# After abstract screening\n",
    "aas_data = pd.read_csv(\"schoot-lgmm-ptsd-included-1.csv\")\n",
    "\n",
    "# Directly included after reading the abstract\n",
    "dir_data = pd.read_csv(\"schoot-lgmm-ptsd-included-3.csv\")\n",
    "\n",
    "# all titles (clean)\n",
    "all_title = all_data[\"title\"].str.replace(\"[^A-Za-z0-9]\", \"\", regex=True)\n",
    "# included titles (clean)\n",
    "inc_title = inc_data[\"title\"].str.replace(\"[^A-Za-z0-9]\", \"\", regex=True)\n",
    "# After abstract screening title\n",
    "aas_title = aas_data[\"title\"].str.replace(\"[^A-Za-z0-9]\", \"\", regex=True)\n",
    "# Directly included\n",
    "dir_title = dir_data[\"title\"].str.replace(\"[^A-Za-z0-9]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers included, missing from initial data:               0 \n",
      "\n",
      "Papers in abstract screening, missing from initial data:  2 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inc_missing = ~inc_title.str.lower().isin(all_title.str.lower())\n",
    "aas_missing = ~aas_title.str.lower().isin(all_title.str.lower())\n",
    "\n",
    "print(\"Papers included, missing from initial data:              \", inc_missing.sum(), \"\\n\")\n",
    "print(\"Papers in abstract screening, missing from initial data: \", aas_missing.sum(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing papers to original dataset.\n",
    "all_data = all_data.append(aas_data[aas_missing], sort=True).reset_index(drop=True)\n",
    "all_data = all_data.append(inc_data[inc_missing], sort=True).reset_index(drop=True)\n",
    "\n",
    "# Update \"cleaned\" titles with new additions.\n",
    "all_title = all_data[\"title\"].str.replace(\"[^A-Za-z0-9]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers with missing title:               64 \n",
      "\n",
      "Number of papers with missing abstract:            764 \n",
      "\n",
      "Number of papers with missing title AND abstract:  62 \n",
      "\n",
      "Number of papers with missing title OR abstract:   766 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "label_inc = all_title.str.lower().isin(inc_title.str.lower())\n",
    "label_aas = all_title.str.lower().isin(aas_title.str.lower())\n",
    "label_dir = all_title.str.lower().isin(dir_title.str.lower())\n",
    "\n",
    "CODE_EXCLUDE = 0      # Exclude paper.\n",
    "CODE_AFT_EXCLUDE = 1  # Exclude after reading full text.\n",
    "CODE_AFT_INCLUDE = 2  # Include after reading full text.\n",
    "CODE_AAS_INCLUDE = 3  # Include after reading abstract.\n",
    "\n",
    "inclusion_code = (label_aas & ~label_inc)*CODE_AFT_EXCLUDE\n",
    "inclusion_code = inclusion_code + (label_inc & ~label_dir)*CODE_AFT_INCLUDE\n",
    "inclusion_code = inclusion_code + label_dir*CODE_AAS_INCLUDE\n",
    "\n",
    "train_data = all_data.assign(included = label_inc.astype(int), inclusion_code = inclusion_code)\n",
    "train_data = train_data.assign(authors = train_data['authors'].str.replace(\"[\\\\[']\", \"\", regex=True))\n",
    "train_data = train_data.assign(authors = train_data['authors'].str.replace(\"\\\\]\", \"\", regex=True))\n",
    "\n",
    "# 64 with missing title\n",
    "print(\"Number of papers with missing title:              \", all_title.isna().sum(), \"\\n\")\n",
    "\n",
    "# 762 with missing abstract\n",
    "print(\"Number of papers with missing abstract:           \", all_data['abstract'].isna().sum(), \"\\n\")\n",
    "\n",
    "# 62 with both missing titles and abstracts\n",
    "print(\"Number of papers with missing title AND abstract: \", (all_title.isna() & all_data['abstract'].isna()).sum(), \"\\n\")\n",
    "\n",
    "# 764 with either missing titles or abstracts\n",
    "print(\"Number of papers with missing title OR abstract:  \", (all_title.isna() | all_data['abstract'].isna()).sum(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining papers in training set:      5782 \n",
      "\n",
      "Total number of INCLUSIONS:                  38  ( 0.66 % )\n",
      "\n",
      "Total number of EXCLUSIONS:                  5744 \n",
      "\n",
      "\n",
      "Total EXCLUSIONS after abstract screening:   5426 \n",
      "\n",
      "Total EXCLUSIONS after full text screening:  318 \n",
      "\n",
      "Total INCLUSIONS after full text screening:  30 \n",
      "\n",
      "Total INCLUSIONS after abstract screening:   8 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates based on just titles\n",
    "unique_train_data = train_data[(~all_title.str.lower().duplicated()) | all_title.str.lower().isnull()]\n",
    "\n",
    "n_train = len(unique_train_data['included'])\n",
    "n_inc = unique_train_data['included'].sum()\n",
    "n_exc = n_train-n_inc\n",
    "\n",
    "n_aas_exc = (unique_train_data['inclusion_code'] == 0).sum()\n",
    "n_aft_exc = (unique_train_data['inclusion_code'] == CODE_AFT_EXCLUDE).sum()\n",
    "n_aft_inc = (unique_train_data['inclusion_code'] == CODE_AFT_INCLUDE).sum()\n",
    "n_aas_inc = (unique_train_data['inclusion_code'] == CODE_AAS_INCLUDE).sum()\n",
    "\n",
    "print(\"Total remaining papers in training set:     \", n_train, \"\\n\")\n",
    "print(\"Total number of INCLUSIONS:                 \", n_inc, \" (\", round(100*n_inc/n_exc, 2), \"% )\\n\")\n",
    "print(\"Total number of EXCLUSIONS:                 \", n_exc, \"\\n\\n\")\n",
    "print(\"Total EXCLUSIONS after abstract screening:  \", n_aas_exc, \"\\n\")\n",
    "print(\"Total EXCLUSIONS after full text screening: \", n_aft_exc, \"\\n\")\n",
    "print(\"Total INCLUSIONS after full text screening: \", n_aft_inc, \"\\n\")\n",
    "print(\"Total INCLUSIONS after abstract screening:  \", n_aas_inc, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_train_data.to_csv(\"PTSD_VandeSchoot_18.csv\", index=False)\n",
    "os.rename(os.path.join(\"/automated-systematic-review-datasets/datasets/Van_de_Schoot_PTSD/csv\", \"PTSD_VandeSchoot_18.csv\"),\n",
    "          os.path.join(\"/automated-systematic-review-datasets/datasets/Van_de_Schoot_PTSD/output\", \"PTSD_VandeSchoot_18.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
